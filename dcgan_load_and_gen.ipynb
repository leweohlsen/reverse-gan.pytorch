{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from dataset import get_dataloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcgan import NetG, NetD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "nc = 3\n",
    "batch_size = 1\n",
    "\n",
    "# load netG\n",
    "netG = NetG(ngpu, nz, ngf, nc)\n",
    "netG.load_state_dict(torch.load('dcgan_out/netG_epoch_10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ground-truth noise\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "noise = Variable(noise)\n",
    "noise.data.resize_(batch_size, nz, 1, 1)\n",
    "noise.data.normal_(0, 1)\n",
    "\n",
    "for param in netG.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# generate ground-truth fake\n",
    "fake = netG(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0\t steps is 0.350173681974411\n",
      "Loss after 1\t steps is 0.22768241167068481\n",
      "Loss after 2\t steps is 0.15123112499713898\n",
      "Loss after 3\t steps is 0.11117040365934372\n",
      "Loss after 4\t steps is 0.09148666262626648\n",
      "Loss after 5\t steps is 0.08174312859773636\n",
      "Loss after 6\t steps is 0.07539718598127365\n",
      "Loss after 7\t steps is 0.07017917186021805\n",
      "Loss after 8\t steps is 0.06603188812732697\n",
      "Loss after 9\t steps is 0.06259655207395554\n",
      "Loss after 10\t steps is 0.05956171825528145\n",
      "Loss after 11\t steps is 0.05643821880221367\n",
      "Loss after 12\t steps is 0.05331909656524658\n",
      "Loss after 13\t steps is 0.05074559524655342\n",
      "Loss after 14\t steps is 0.048578858375549316\n",
      "Loss after 15\t steps is 0.046721428632736206\n",
      "Loss after 16\t steps is 0.04515514150261879\n",
      "Loss after 17\t steps is 0.04377645626664162\n",
      "Loss after 18\t steps is 0.04258820414543152\n",
      "Loss after 19\t steps is 0.04149062559008598\n",
      "Loss after 20\t steps is 0.04047531634569168\n",
      "Loss after 21\t steps is 0.03951046243309975\n",
      "Loss after 22\t steps is 0.0385311059653759\n",
      "Loss after 23\t steps is 0.03763989731669426\n",
      "Loss after 24\t steps is 0.03682129085063934\n",
      "Loss after 25\t steps is 0.03603784367442131\n",
      "Loss after 26\t steps is 0.03525461629033089\n",
      "Loss after 27\t steps is 0.03453446924686432\n",
      "Loss after 28\t steps is 0.03380713239312172\n",
      "Loss after 29\t steps is 0.03308238834142685\n",
      "Loss after 30\t steps is 0.03236481919884682\n",
      "Loss after 31\t steps is 0.0316588394343853\n",
      "Loss after 32\t steps is 0.030980629846453667\n",
      "Loss after 33\t steps is 0.030320318415760994\n",
      "Loss after 34\t steps is 0.02970670722424984\n",
      "Loss after 35\t steps is 0.029085630550980568\n",
      "Loss after 36\t steps is 0.028489327058196068\n",
      "Loss after 37\t steps is 0.027875661849975586\n",
      "Loss after 38\t steps is 0.027251943945884705\n",
      "Loss after 39\t steps is 0.02666751854121685\n",
      "Loss after 40\t steps is 0.02612713724374771\n",
      "Loss after 41\t steps is 0.02561747469007969\n",
      "Loss after 42\t steps is 0.025132762268185616\n",
      "Loss after 43\t steps is 0.02465665340423584\n",
      "Loss after 44\t steps is 0.024176424369215965\n",
      "Loss after 45\t steps is 0.023729747161269188\n",
      "Loss after 46\t steps is 0.023298755288124084\n",
      "Loss after 47\t steps is 0.022886192426085472\n",
      "Loss after 48\t steps is 0.022485829889774323\n",
      "Loss after 49\t steps is 0.022089272737503052\n"
     ]
    }
   ],
   "source": [
    "# fix fake, and try to find noise_approx\n",
    "noise_approx = Variable(torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1))\n",
    "noise_approx.requires_grad = True\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer_approx = optim.Adam([noise_approx], lr=0.1, betas=(0.5, 0.999))\n",
    "\n",
    "for i in range(50):\n",
    "    fake_approx = netG(noise_approx)\n",
    "    loss = mse_loss(fake_approx, fake)\n",
    "    # mse_z = mse_loss(noise_approx, noise)\n",
    "    print(\"Loss after {}\\t steps is {}\".format(i, loss.data[0]))\n",
    "    \n",
    "    optimizer_approx.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_approx.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fake = netG(noise)\n",
    "fake_approx = netG(noise_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vutils.save_image(fake.data, 'fake.png', normalize=True)\n",
    "vutils.save_image(fake_approx.data, 'fake_approx.png', normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
